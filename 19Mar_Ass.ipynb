{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c2a6fc-9312-4250-abab-de677958c3c7",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd28406-d544-4984-86ce-241b140ab2cf",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that rescales the values of features in a dataset to a common range, typically between 0 and 1. This is done to ensure that all features have a similar scale, which can help to improve the performance of machine learning algorithms.\n",
    "\n",
    "Here are some of the benefits of using Min-Max scaling:\n",
    "\n",
    "It is a simple and easy-to-implement technique.\n",
    "\n",
    "It can be used with any type of data.\n",
    "\n",
    "It can help to improve the performance of machine learning algorithms.\n",
    "\n",
    "\n",
    "However, there are also some limitations to Min-Max scaling:\n",
    "\n",
    "It can amplify the effects of outliers.\n",
    "\n",
    "It can make the data less interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df59b333-6748-4952-9532-61bb04ee6697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33333333],\n",
       "       [0.09090909, 0.        ],\n",
       "       [0.54545455, 0.26666667],\n",
       "       [1.        , 1.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "import pandas as pd\n",
    "data = {'age':[23,25,35,45],'weight':[135,125,133,155]}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805ae54-22b5-4ac1-b3f9-190e4295c05b",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a795e4a-eff8-413f-979e-e97f43a070cf",
   "metadata": {},
   "source": [
    "The unit vector technique is a feature scaling technique that rescales the values of features in a dataset to have a unit length. This means that the Euclidean norm of each feature vector will be equal to 1.\n",
    "\n",
    "Unit vector scaling is similar to Min-Max scaling in that it ensures that all features have a similar scale. However, unit vector scaling does this by normalizing the length of each feature vector, rather than by rescaling the values of the features to a common range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3604851a-e630-4ab4-9a08-861f31233413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16795034, 0.98579546],\n",
       "       [0.19611614, 0.98058068],\n",
       "       [0.2544933 , 0.96707454],\n",
       "       [0.27881019, 0.96034623]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "from sklearn.preprocessing import normalize\n",
    "normalize_data = normalize(df)\n",
    "normalize_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadf29d-1186-4958-acfb-de27fc743163",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of correlated variables into a set of uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. PCA is a widely used technique for dimensionality reduction in machine learning and data analysis.\n",
    "\n",
    "feature1 | feature2 | feature3\n",
    "------- | -------- | --------\n",
    "1.0 | 0.5 | 0.2\n",
    "0.5 | 1.0 | 0.3\n",
    "0.2 | 0.3 | 1.0\n",
    "\n",
    "\n",
    "The three features in this dataset are all correlated. This means that they tend to vary together. For example, if feature1 is high, then feature2 and feature3 are also likely to be high.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of this dataset by finding a new set of features that are uncorrelated. These new features are called principal components. The first principal component will capture the most variation in the data, the second principal component will capture the second most variation in the data, and so on.\n",
    "\n",
    "In this example, the first principal component will be a linear combination of feature1, feature2, and feature3 that captures the most variation in the data. The second principal component will be a linear combination of feature1, feature2, and feature3 that captures the second most variation in the data, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be0a04-42e4-47f0-8b98-34d287a63850",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a89c3-e548-44c6-a1f9-7d476ed7f101",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a statistical procedure that can be used to reduce the dimensionality of a dataset while preserving as much information as possible. This is done by finding a set of new features that are uncorrelated and that capture the most variation in the original dataset.\n",
    "\n",
    "Feature extraction is the process of transforming a dataset of raw data into a new set of features that are more relevant to the problem at hand. This can be done using a variety of techniques, including PCA.\n",
    "\n",
    "PCA can be used for feature extraction by selecting the principal components that capture the most variation in the data. These principal components can then be used as features for machine learning algorithms.\n",
    "\n",
    "For example, let's say we have a dataset of images of handwritten digits. Each image is represented as a 28x28 pixel matrix, which means that the dataset has 784 features. We can use PCA to reduce the dimensionality of this dataset to 100 features by selecting the principal components that capture the most variation in the data. These 100 principal components can then be used as features for a machine learning algorithm that classifies handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb7bf9-a61b-45e7-b13d-eb3132ef0dbc",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0bcfb-eb86-42b5-afdd-ffcc790efdf7",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that rescales the values of features in a dataset to a common range, typically between 0 and 1. This is done to ensure that all features have a similar scale, which can help to improve the performance of machine learning algorithms.\n",
    "\n",
    "In the case of a food delivery service recommendation system, the features that you mentioned (price, rating, and delivery time) are all on different scales. For example, price could range from $1 to $100, rating could range from 1 to 5 stars, and delivery time could range from 15 minutes to 2 hours.\n",
    "\n",
    "If you use these features as-is, the machine learning algorithm may not be able to learn effectively. This is because the algorithm may give more weight to features that are on a larger scale, such as price.\n",
    "\n",
    "Min-Max scaling can help to address this problem by rescaling the values of all features to a common range. This will ensure that all features have a similar weight, which will help the machine learning algorithm to learn more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0413ace-dc8b-4118-992e-92b935b09ca1",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86001616-a690-4fb5-a57c-481b22b5a916",
   "metadata": {},
   "source": [
    "PCA is a dimensionality reduction technique that can be used to reduce the number of features in a dataset while preserving as much information as possible. This can be useful for machine learning tasks, such as stock price prediction, where the number of features can be large and noisy.\n",
    "\n",
    "To use PCA to reduce the dimensionality of a stock price prediction dataset, you can use the following steps:\n",
    "\n",
    "1. Import the PCA class from the sklearn.decomposition library.\n",
    "\n",
    "2. Create a PCA object.\n",
    "\n",
    "3. Fit the PCA object to the dataset.\n",
    "\n",
    "4. Select the number of principal components to keep.\n",
    "\n",
    "5. Transform the dataset using the PCA object.\n",
    "\n",
    "The output of the transform() method is a new dataset with the number of features reduced to n_components. This will help the machine learning algorithm to learn more effectively, as it will be working with a smaller and less noisy dataset.\n",
    "\n",
    "In addition to reducing the dimensionality of the dataset, PCA can also be used to identify the most important features. This can be done by examining the eigenvalues of the PCA object. The eigenvalues represent the amount of variance that is captured by each principal component. The principal components with the largest eigenvalues are the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3fb27-54cc-41ad-be51-5b6ee8263e1b",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40582e2-742e-42e0-be66-487e490a6579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the minimum and maximum values in the dataset.\n",
    "min_value = np.min([1, 5, 10, 15, 20])\n",
    "max_value = np.max([1, 5, 10, 15, 20])\n",
    "\n",
    "# Subtract the minimum value from each value in the dataset.\n",
    "scaled_dataset = [x - min_value for x in [1, 5, 10, 15, 20]]\n",
    "\n",
    "# Divide each value by the difference between the minimum and maximum values.\n",
    "scaled_dataset = [x / (max_value - min_value) for x in scaled_dataset]\n",
    "\n",
    "# Multiply each value by 2 and then subtract 1.\n",
    "scaled_dataset = [2 * x - 1 for x in scaled_dataset]\n",
    "\n",
    "print(scaled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057c956-2ac7-471d-a4f0-58f463141ff6",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5541da-5769-4d9f-bd94-f574ac4f2851",
   "metadata": {},
   "source": [
    "The number of principal components to retain depends on the specific dataset and the task at hand. In general, you would want to retain as many principal components as possible while still ensuring that the new dataset is representative of the original dataset.\n",
    "\n",
    "In this case, the features [height, weight, age, gender, blood pressure] are all likely to be important for predicting health outcomes. Therefore, I would choose to retain all 5 principal components.\n",
    "\n",
    "However, if you were only interested in predicting a specific health outcome, such as heart disease, you might be able to get away with retaining fewer principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d6f65b-6622-4a09-ad90-17b662588cd0",
   "metadata": {},
   "source": [
    "import sklearn.decomposition \n",
    "\n",
    " Create a PCA object.\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "  Fit the PCA object to the dataset.\n",
    "pca.fit(dataset)\n",
    "\n",
    " Select the number of principal components to keep.\n",
    "n_components = 2\n",
    "\n",
    "  Transform the dataset using the PCA object.\n",
    "scaled_dataset = pca.transform(dataset)\n",
    "\n",
    "this code can perform the feature extraction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef5131-2474-4746-a28a-7af0ecbd7996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
