{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e014b9ec-dca9-4d6b-8184-98d3f99a14da",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994bda1-87ef-4751-8e25-04a9ba393da1",
   "metadata": {},
   "source": [
    "Gradient boosting regression is a machine learning algorithm that combines multiple weak learners to create a strong learner. The weak learners are typically decision trees, but they can also be other types of models.\n",
    "\n",
    "The basic idea behind gradient boosting regression is to start with a simple model, such as a mean predictor, and then iteratively add new models that focus on the errors made by the previous models. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble. The gradient represents the direction in which the predictions need to be improved. A new model is then trained to minimize the gradient.\n",
    "\n",
    "The process is repeated until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "The final predictions of the ensemble are made by combining the predictions of the weak learners. The combination is typically done by averaging the predictions.\n",
    "\n",
    "Gradient boosting regression is a powerful algorithm that can be used for a variety of machine learning tasks, including regression, classification, and ranking. It is a good choice for problems where the data is noisy or where the underlying relationships are complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfffd0dc-458f-4d84-b6e4-ce47e2097314",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0893ef75-5718-488b-8e9c-b99cb9476296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ba09b0-63fb-4c90-91d2-16949f56ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "x,y = make_regression(n_samples=800,n_features=4,n_informative=2,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9431c45f-5d80-474d-9bea-8fe45dfb633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6416eda9-52f3-4765-8e25-e1e983fd8dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((560, 4), (240, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "488d3d92-eaee-4b09-bd9d-e8952e71f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grb = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79b2351-a5da-483a-8fe8-0908cb87a93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09893b8a-66d5-42c3-9754-58fe4a02a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=grb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2699f1d-1651-4a48-a4d3-41cc22c919e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ad6e39-2195-469e-b0a7-9298ed9e8de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.60082212095326"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabfdaaa-4e20-41ec-96eb-055b2a365888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945517070901666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdbf97-a72e-4830-8f3e-49f401e5377c",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "796b33a3-f7aa-4047-a581-91cb3ba5ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'loss':['squared_error', 'absolute_error'],'learning_rate':[0.1,0.2,0.11],'n_estimators':[100,110,120]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40e6e826-26ed-4a9e-b58b-8f0e8cfb3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv = GridSearchCV(grb,param_grid=parameters,cv=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9b9def1-9ddd-48b9-b789-b4dad97ece5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.2, 0.11],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [100, 110, 120]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.2, 0.11],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [100, 110, 120]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.11],\n",
       "                         'loss': ['squared_error', 'absolute_error'],\n",
       "                         'n_estimators': [100, 110, 120]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fac6a2c-9c27-4e0e-a16a-36ce73bf5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=gcv.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "852eaefc-2717-4881-aade-8a278f607ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.136807195199435"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d510bcf-43a8-498a-92a5-57263bafddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994164291437986"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5d34ffb-99ea-40a1-a626-0b620091a706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.11, 'loss': 'squared_error', 'n_estimators': 120}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbac8be-2b8d-493b-8d55-92f338742911",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb50b29-dfdc-4959-b8c1-4732d277a019",
   "metadata": {},
   "source": [
    "\n",
    "In the context of gradient boosting, a weak learner is a machine learning model that is only slightly better than random guessing. Weak learners are typically decision trees, but they can also be other types of models.\n",
    "\n",
    "The goal of gradient boosting is to combine a set of weak learners to create a strong learner. The weak learners are trained sequentially, and each new learner is trained to correct the errors made by the previous learners.\n",
    "\n",
    "The gradient boosting algorithm starts with a simple model, such as a mean predictor. Then, it iteratively adds new models that focus on the errors made by the previous models. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble. The gradient represents the direction in which the predictions need to be improved. A new model is then trained to minimize the gradient.\n",
    "\n",
    "The process is repeated until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "The final predictions of the ensemble are made by combining the predictions of the weak learners. The combination is typically done by averaging the predictions.\n",
    "\n",
    "Weak learners are important in gradient boosting because they allow the algorithm to learn from its mistakes. By iteratively adding new weak learners, the algorithm can gradually improve its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ed245-58b3-44ad-b9c4-604bd320d23a",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085409e-9c01-4f76-a39e-3ff5e486ce23",
   "metadata": {},
   "source": [
    "The intuition behind the gradient boosting algorithm is to build an ensemble of weak learners that collectively make better predictions than any individual learner. The weak learners are typically decision trees, but they can also be other types of models.\n",
    "\n",
    "The algorithm starts with a simple model, such as a mean predictor. Then, it iteratively adds new models that focus on the errors made by the previous models. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble. The gradient represents the direction in which the predictions need to be improved. A new model is then trained to minimize the gradient.\n",
    "\n",
    "The process is repeated until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "The final predictions of the ensemble are made by combining the predictions of the weak learners. The combination is typically done by averaging the predictions.\n",
    "\n",
    "Here is an example of how gradient boosting works for a regression problem. Let's say we are trying to predict the price of a house. We start with a simple model, such as a mean predictor. This model predicts that all houses will sell for the same price.\n",
    "\n",
    "In the next iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the mean predictor. The gradient represents the direction in which the predictions need to be improved. In this case, the gradient will be pointing towards the houses that are more expensive than the mean price.\n",
    "\n",
    "A new model is then trained to minimize the gradient. This model will learn to predict higher prices for houses that are more expensive than the mean price.\n",
    "\n",
    "The process is repeated until a stopping criterion is met. The final predictions of the ensemble are made by averaging the predictions of the weak learners.\n",
    "\n",
    "The intuition behind gradient boosting is that the weak learners can learn from each other. The first weak learner learns to predict the average price of a house. The second weak learner learns to predict the prices of houses that are more expensive than the mean price. The third weak learner learns to predict the prices of houses that are even more expensive than the mean price, and so on.\n",
    "\n",
    "By iteratively adding new weak learners, the algorithm can gradually improve its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e35f78-f50d-46d3-ac58-e7a068af8f42",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06fefd2-9d94-4787-87e0-978e0a2f48cb",
   "metadata": {},
   "source": [
    "Gradient boosting builds an ensemble of weak learners by iteratively adding new learners that focus on the errors made by the previous learners. The algorithm starts with a simple model, such as a mean predictor. Then, it iteratively adds new models that focus on the errors made by the previous models. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble. The gradient represents the direction in which the predictions need to be improved. A new model is then trained to minimize the gradient.\n",
    "\n",
    "The process is repeated until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "The final predictions of the ensemble are made by combining the predictions of the weak learners. The combination is typically done by averaging the predictions.\n",
    "\n",
    "Here is an example of how gradient boosting builds an ensemble of weak learners for a regression problem. Let's say we are trying to predict the price of a house. We start with a simple model, such as a mean predictor. This model predicts that all houses will sell for the same price.\n",
    "\n",
    "In the next iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the mean predictor. The gradient represents the direction in which the predictions need to be improved. In this case, the gradient will be pointing towards the houses that are more expensive than the mean price.\n",
    "\n",
    "A new model is then trained to minimize the gradient. This model will learn to predict higher prices for houses that are more expensive than the mean price.\n",
    "\n",
    "The process is repeated until a stopping criterion is met. The final predictions of the ensemble are made by averaging the predictions of the weak learners.\n",
    "\n",
    "The weak learners are typically decision trees, but they can also be other types of models. The decision trees are trained to minimize the loss function of the ensemble. The loss function is typically the mean squared error (MSE) for regression problems and the cross-entropy loss for classification problems.\n",
    "\n",
    "The gradient boosting algorithm is a powerful algorithm that can be used to solve a variety of machine learning problems. It is relatively easy to implement and tune, and it can be used to deal with missing values and outliers. However, it can be computationally expensive, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70125651-3035-4e76-bbad-aefca548a128",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5db082-6eee-4a4e-b425-b38bcf83ff52",
   "metadata": {},
   "source": [
    "The mathematical intuition of gradient boosting algorithm can be constructed in the following steps:\n",
    "\n",
    "Define a loss function that measures the error between the predicted and true values.\n",
    "\n",
    "Initialize the predictions to a constant value.\n",
    "\n",
    "For each iteration:\n",
    "\n",
    "Compute the gradient of the loss function with respect to the predictions.\n",
    "\n",
    "Train a weak learner to minimize the gradient.\n",
    "\n",
    "Add the predictions of the weak learner to the existing predictions.\n",
    "\n",
    "Repeat steps 3 and 4 until a stopping criterion is met.\n",
    "\n",
    "The loss function is typically the mean squared error (MSE) for regression problems and the cross-entropy loss for classification problems. The weak learners are typically decision trees, but they can also be other types of models.\n",
    "\n",
    "The gradient of the loss function represents the direction in which the predictions need to be improved. The weak learner is trained to minimize the gradient, which means that it will learn to predict the values that are closest to the true values.\n",
    "\n",
    "The predictions of the weak learner are then added to the existing predictions. This means that the predictions are iteratively updated to become more accurate.\n",
    "\n",
    "The stopping criterion can be a maximum number of iterations, a minimum improvement in the loss function, or a maximum value of the gradient.\n",
    "\n",
    "The mathematical intuition of gradient boosting algorithm can be used to understand how the algorithm works and to choose the hyperparameters that will lead to the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb3b37-505b-4b4d-ab91-6a1581103c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
