{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4873a60-2a19-4672-8977-63d899055ace",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9bee7-e056-4bf5-b401-f0482bc47a06",
   "metadata": {},
   "source": [
    "Overfitting:\n",
    "\n",
    "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
    "\n",
    "How to avoid the Overfitting in Model\n",
    "\n",
    "Cross-Validation\n",
    "\n",
    "\n",
    "Training with more data\n",
    "\n",
    "\n",
    "Removing features\n",
    "\n",
    "\n",
    "Early stopping the training\n",
    "\n",
    "Underfitting:\n",
    "\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "\n",
    "How to avoid underfitting:\n",
    "\n",
    "By increasing the training time of the model.\n",
    "\n",
    "By increasing the number of features.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ada35f-84f8-4dde-b558-05caa1acb412",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a3f1d-d2eb-405c-ba92-9e6e57fafda6",
   "metadata": {},
   "source": [
    "Use Dropouts. Dropout is a regularization technique that prevents neural networks from overfitting. Regularization methods like L1 and L2 reduce overfitting by modifying the cost function. Dropout on the other hand, modify the network itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddffae0a-28b3-4cff-8d6a-6904622f63ce",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aea4d8-0691-44f8-84ee-af0ec9ce98d1",
   "metadata": {},
   "source": [
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "\n",
    "\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce4544-93de-4d07-b7e0-4b01327b9040",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da554f8b-190d-4f96-976d-514c166f5e4f",
   "metadata": {},
   "source": [
    "What is Bias?\n",
    "\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as the Underfitting of Data.\n",
    "\n",
    "\n",
    "What is Variance?\n",
    "\n",
    "\n",
    "The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data.\n",
    "\n",
    "\n",
    "“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff. Target Function. An ideal algorithm should neither underfit nor overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15cafb-7156-4fef-80dd-981ab3e154bc",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca0064-edcd-4027-8923-6abbd5dd3ce7",
   "metadata": {},
   "source": [
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model performs poorly on the training data.\n",
    "\n",
    "Low bias and large variation: overfitting occurs when the algorithm produces widely diverse predictions for the same data. High bias, low variance: underfitting occurs when the algorithm produces comparable predictions for similar data, but the predictions are incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b7d28-44b9-46d4-84bf-6b4f6efe7113",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46387cc-0040-4fda-bea9-0136215733c1",
   "metadata": {},
   "source": [
    "-Examples of low-bias machine learning algorithms include Decision Trees, k-Nearest Neighbors and Support Vector Machines. -Examples of high-bias machine learning algorithms include Linear Regression, Linear Discriminant Analysis, and Logistic Regression.\n",
    "\n",
    "\n",
    "What is Bias?\n",
    "\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as the Underfitting of Data.\n",
    "\n",
    "\n",
    "What is Variance?\n",
    "\n",
    "\n",
    "The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data.\n",
    "\n",
    "\n",
    "“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff. Target Function. An ideal algorithm should neither underfit nor overfit the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563defa5-21f3-4a8a-a2ad-607c197c008c",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8d3ae-d7ac-4ab7-9a5d-00f986ce8c4a",
   "metadata": {},
   "source": [
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "\n",
    "There are three main regularization techniques, namely:\n",
    "\n",
    "Ridge Regression (L2 Norm)\n",
    "\n",
    "Lasso (L1 Norm)\n",
    "\n",
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e37fa-87a5-4324-b8e4-b260524949b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
